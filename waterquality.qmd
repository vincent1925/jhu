title: "Water Quality Draft"
author: "Vincent Zheng"
date: 6/25/2024"
latex-auto-mk: true
output: html_document
format:
  html:
    code-fold: show
    code-tools: 
      source: true
      toggle: true

```{python}

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from linearmodels import PanelOLS
from census import Census
import statsmodels.api as sm
import statsmodels.formula.api as smf

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 60)
pd.set_option("display.float_format", lambda x: "{:.5f}".format(x))

plt.style.use("ggplot")

PATH = "/Users/zheng/Downloads/Internships:Work/water.csv"

# water = pd.read_csv(PATH)

```

# issues
- the number of watersystems in water data is different from the number of watersystems in water_geo data
_ the number of zip codes in zip_geo is different from the mortality data. mortality data has 2664 for some reason
- in analyte water, not every water system has a result for each year

before i ran the regressions, i was going over my code to make sure i didn't make any bad assumptions mistakes. i remembered that i set all suppressed mortality count data to 0 so i could move on. how should i approach this? i know there are packages in r to simulate suppressed data


# California Water Quality Data

The data is obtained from (https://www.waterboards.ca.gov/drinking_water/certlic/drinkingwater/EDTlibrary.html "this page") from the State Water Resources Control Board from January 1, 2011 to present. However, I'll only be using data up until 2022.

For NaN results in the 'Result' column, I replaced them with the Reporting Level divided by the square root of 2 as is common practice.

For sake of thoroughness, I filled in the 'Less Than Reporting Level' by comparing 'Result' to 'Reporting Level'.

To account for varying units, I identified those analytes that had multiple units of measure and converted them to the most common unit of measure. For those that didn't have a unit of measure, I deduced the unit by using the 'Reporting Level'.

I also dropped the pH values that were greater than 14, assumning that they were errors.

in the meantime, i was reviewing different aspects of the regression. i'm not sure if i should be concerned about imputing nan values for analyte concentration using reportinglevel/sqrt2 because 78% of the data 

```{python}

# Data files
files = [f"california_water_systems/SDWIS{i}.tab" for i in range(1, 5)]

# List of columns to read
cols = ['Water System Number', 'Population Served', 'Sampling Point Name', 'Sample Date', 'Analyte Name', 'Result', 'Counting Error', 'Units of Measure', 'Less Than Reporting Level', 'Reporting Level', 'DLR', 'MCL']

# Read and concatenate all files
water = pd.concat(
    [pd.read_table(file, encoding="ISO-8859-1", usecols=cols, dtype={20: str}) for file in files],
    ignore_index=True
)

# Make a new column in water that gets the year under sample date
water['Sample Date'] = pd.to_datetime(water['Sample Date'], format='%m-%d-%Y')
water['Sample Year'] = water['Sample Date'].dt.year

# Drop rows that are in years 2023 and 2024
water = (water
        .loc[~water["Sample Year"].isin([2023, 2024])]
        .reset_index(drop=True)
)

# Remove trailing whitespace
columns_to_strip = ["Water System Number", "Analyte Name", "Units of Measure", "Result"]
for column in columns_to_strip:
    water[column] = water[column].str.rstrip()

water['Result'] = pd.to_numeric(water['Result'].replace("", np.nan), errors='coerce')

water = (water
        .groupby('Analyte Name')
        .filter(lambda x: x['Result'].notna().any())
)
##### 

nan_percentage = water.groupby('Analyte Name')['Result'].apply(lambda x: x.isnull().mean() * 100)
nan_percentage = (nan_percentage
                  .reset_index()
                  .rename(columns={'Result': 'NaN Percentage'})
                  .sort_values(by='NaN Percentage', ascending=False)
                  .reset_index(drop=True)
)

######## 

water_copy = water.copy()

analyte_names = water['Analyte Name'].unique()
nan_percentages = []

# Calculate the percentage of NaN values after dropping each analyte
for analyte in analyte_names:
    water_copy = water_copy[water_copy['Analyte Name'] != analyte]
    nan_rows_percentage = water_copy['Result'].isna().sum() / len(water) * 100
    nan_percentages.append(nan_rows_percentage)

# Create a DataFrame to hold the results
nan_percentage_df = pd.DataFrame({
    'Analyte': analyte_names,
    'NaN Percentage': nan_percentages
})

#########

analyte_nan = water_copy.groupby('Analyte Name')['Result'].apply(lambda x: x.isnull().sum() )
analyte_nan = analyte_nan.reset_index().rename(columns={'Result': 'NaN Percentage'}).sort_values(by='NaN Percentage', ascending=False).reset_index(drop=True)

analyte_nan.loc[analyte_nan['NaN Percentage'] > 50]

plt.figure(figsize=(12, 8))
plt.bar(analyte_nan['Analyte Name'], analyte_nan['NaN Percentage'], color='skyblue')
plt.xlabel('Analyte')
plt.ylabel('Percentage of NaN Values')
plt.title('Percentage of NaN Values in Result Column by Analyte')
plt.xticks(rotation=45)
plt.show()

water['Result'] = water['Result'].astype(float)

#########


# NaN values under Result are replaced with Reporting Level/sqrt(2)
mask = water['Result'].isnull()
water.loc[mask, 'Result'] = water.loc[mask, 'Reporting Level'] / np.sqrt(2)

# Making 'Result' column float
water['Result'] = water['Result'].astype(float)

# filling in values for 'Less Than Reporting Level' column
conditions = [
    (water['Less Than Reporting Level'] == ' ') & (water['Result'] < water['Reporting Level']),
    (water['Less Than Reporting Level'] == ' ') & (water['Result'] > water['Reporting Level']),
    (water['Less Than Reporting Level'] == ' ') & (water['Result'] == water['Reporting Level'])
]
choices = ['Y', 'N', 'Neither']

water['Less Than Reporting Level'] = np.select(conditions, choices, default=water['Less Than Reporting Level'])

# Function to fix units for a specific analyte
def fix_units(analyte, default_unit, conversion_factor=None):
    if conversion_factor:
        water.loc[(water['Analyte Name'] == analyte) & (water['Units of Measure'] == 'MG/L'), 
                  ['Reporting Level', 'Result', 'DLR', 'MCL']] *= conversion_factor
        water.loc[(water['Analyte Name'] == analyte) & (water['Units of Measure'] == 'MG/L'), 
                  'Units of Measure'] = default_unit
    water.loc[(water['Analyte Name'] == analyte) & (water['Units of Measure'] == ''), # assuming blank units of measure is the one that is most common by checking reporting level
              'Units of Measure'] = default_unit

# Mapping analytes to their default units and conversion factors (if any)

analyte_unit_fixes = {
    'CHLOROFORM': {'default_unit': 'UG/L'},
    'LEAD': {'default_unit': 'UG/L', 'conversion_factor': 1000},
    'COPPER, FREE': {'default_unit': 'UG/L', 'conversion_factor': 1000},
    'NITRITE': {'default_unit': 'UG/L', 'conversion_factor': 1000},
    'NITRATE-NITRITE': {'default_unit': 'UG/L', 'conversion_factor': 1000},
    'AGGRESSIVE INDEX': {'default_unit': 'AGGR'}
} # these analytes were identified to have multiple units of measure

# Applying fixes to each analyte
for analyte, fix_params in analyte_unit_fixes.items():
    fix_units(analyte, **fix_params)

# fixing pH
analyte = 'PH'
water = water[~((water['Analyte Name'] == 'PH') & (water['Result'] > 14))].reset_index(drop=True)

# saving file
water.to_csv("water.csv", index=False)

```

```{python}
water = pd.read_csv("water.csv")

def find_outliers_z_score(data, threshold=3):
    mean = np.mean(data)
    std = np.std(data)
    z_scores = (data - mean) / std
    outliers = np.where(np.abs(z_scores) > threshold)[0]
    return outliers

def find_analyte_outliers_z_score(df, analyte_column='Analyte Name', result_column='Result', threshold=3):
    outliers_dict = {}
    analytes = df[analyte_column].unique()
    
    for analyte in analytes:
        analyte_data = df[df[analyte_column] == analyte][result_column]
        outliers_idx = find_outliers_z_score(analyte_data, threshold)
        outliers = df[df[analyte_column] == analyte].iloc[outliers_idx]
        outliers_dict[analyte] = outliers
    
    return outliers_dict

# Assuming `water` is your DataFrame
analyte_outliers = find_analyte_outliers_z_score(water)

def convert_dict_to_df(analyte_outliers):
    outliers_list = []
    
    for analyte, outliers in analyte_outliers.items():
        if not outliers.empty:
            outliers['Analyte Name'] = analyte
            outliers_list.append(outliers)
    
    if outliers_list:
        combined_outliers_df = pd.concat(outliers_list, ignore_index=True)
    else:
        combined_outliers_df = pd.DataFrame()
    
    return combined_outliers_df

# Convert the analyte_outliers dict to a single DataFrame
analyte_outliers_df = convert_dict_to_df(analyte_outliers)

analyte_outliers_df.loc[analyte_outliers_df['Analyte Name'] == 'PERFLUOROPENTANESULFONIC ACID (PFPES)']
import matplotlib.pyplot as plt

# Select the data for the specific analyte
pfpes_data = water.loc[water['Analyte Name'] == 'PERFLUOROPENTANESULFONIC ACID (PFPES)']

# Plot the histogram
for analyte in analyte_outliers_df['Analyte Name'].unique():
    analyte_data = water.loc[water['Analyte Name'] == analyte]
    plt.hist(analyte_data['Result'], bins=10)
    plt.xlabel('Result')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of {analyte} Result')
    plt.show()


```

# California Mortality Data
The mortality data is obtained from (https://data.ca.gov/dataset/death-profiles-by-zip-code "this page") from the California Open Data Portal. The data is from 2009 to 2022 but the years 2009 and 2010 were dropped. 

There are NaN values in the dataset because some data were suppressed for small numbers (<11). To deal with this, I wrote code that allowed you to select to put values with 0 or the mean, 5. When imputing with 5, one must make sure that the mortality count is lower than the population.

I focused on deaths from all causes. Under that, I separated the data into three groups: age, total population, and gender.

For the age group, I combined the subcategories <1 year and 1-4 years into 'Under 5 years' to match the popualation data that I have.

```{python}

cols = ['Year','ZIP_Code','Strata','Strata_Name','Cause','Cause_Desc','Count','Annotation_Code','Annotation_Desc' ] # dropped 'Annotation_Code','Annotation_Desc','Data_Revision_Date','ICD_Revision', 'Geography_Type'
mortality1 = pd.read_csv("california_mortality/cali_deaths_2009-2018.csv", usecols=cols)
mortality2 = pd.read_csv("california_mortality/cali_deaths_2019-2022.csv", usecols=cols)
mortality = pd.concat([mortality1, mortality2], ignore_index=True)

# filtering out causes from mortality 
cause_list = ["ALZ", "CAN", "CLD", "DIA", "HOM", "HTD", "HYP", "INJ", "LIV", "NEP", "PAR", "PNF", "STK", "SUI"]
mortality = (mortality
            .loc[~mortality["Cause"].isin(cause_list)]
            .loc[~mortality["Year"].isin([2009, 2010])] # drop rows that are in years 2009 and 2010
            .rename(columns={"ZIP_Code": "ZIP_CODE"})
            .reset_index(drop=True)
)

# imputing NaN values
def handle_nan_values(mortality, method):
  if method == 'drop':
    # Drop rows with NaN values
    nan_pairs = mortality[mortality['Count'].isna()][['Year', 'ZIP_CODE']]
    mortality = (mortality
                .merge(nan_pairs, on=['Year', 'ZIP_CODE'], how='left', indicator=True)
                .loc[lambda x: x['_merge'] == 'left_only']
                .drop(columns=['_merge'])
    )
  elif method == 'impute_zero':
    # Impute NaN values with 0
    mortality["Count"] = mortality["Count"].fillna(0).astype(int)
  elif method == 'impute_five':
    # Impute NaN values with 5
    mortality["Count"] = mortality["Count"].fillna(5).astype(int)
  else:
    raise ValueError("Invalid method. Choose from 'drop', 'impute_zero', or 'impute_five'.")
  
  return mortality

mortality = handle_nan_values(mortality, 'impute_zero')

# mortality with only age as strata 
strata_map = {
    'Under 1 year': 'Under 5 years',
    '1-4 years': 'Under 5 years', 
    
}
mortality_age = (mortality
                .loc[mortality["Strata"] == "Age"]
                .drop(columns = ['Strata', 'Cause', 'Cause_Desc'])
                .reset_index(drop=True)
                .replace({'Strata_Name': strata_map})
                .groupby(['Year', 'ZIP_CODE','Strata_Name'], as_index=False, sort = False)['Count']
                .sum()
)

# mortality with total population as strata
mortality_total = (mortality
                  .loc[mortality["Strata"] == "Total Population"]
                  .drop(columns=["Strata", "Cause", "Cause_Desc"])
                  .sort_values(by="ZIP_CODE", ascending=True)
                  .reset_index(drop=True)
)

# mortality with gender as strata
mortality_gender = (mortality
                    .loc[mortality["Strata"] == "Gender"]
                    .reset_index(drop=True)
)

```

# Mapping water systems to the zip codes they serve
```{python}

water_geo = gpd.read_file('California_Drinking_Water_System_Area_Boundaries/California_Drinking_Water_System_Area_Boundaries.shp')
zip_geo = gpd.read_file('california_zip_codes/California_Zip_Codes.shp')

# keeping only the relevant columns
water_geo = water_geo.loc[:, ["WATER_SYST", "geometry"]] 
zip_geo = zip_geo.loc[:, ["ZIP_CODE", "geometry"]]
  # note that water_geo has 4798 rows instead of 4776 rows because of water systems that have multiple geometries. this is solved when intersecting with zip_geo which combines the zip codes

# repairing invalid geometries which solves the nan values when sjoining
water_geo['geometry'] = water_geo['geometry'].make_valid()
zip_geo['geometry'] = zip_geo['geometry'].make_valid()

# changing type
zip_geo['ZIP_CODE'] = zip_geo['ZIP_CODE'].astype(int)

# ensures same mapping
zip_geo = zip_geo.to_crs(water_geo.crs)

# the zip codes that each water system serves
water_zip = gpd.sjoin(water_geo, zip_geo, how='inner', predicate='intersects')
water_zip = water_zip.drop(columns=['index_right', 'geometry']).reset_index(drop=True)
water_zip["ZIP_CODE"] = water_zip["ZIP_CODE"].astype(int)

# seperate data frame with grouped zip codes
water_zip_grouped = water_zip.groupby('WATER_SYST')['ZIP_CODE'].apply(lambda x: ', '.join(str(x))).reset_index()


``` 

# Getting the population data to complement the mortality data

```{python}

c = Census("9155f8ce1852424ae314d4653b3a8730bb1b87c1")

def calculate_population_by_age_per_sex(given_sex, given_year):
  
  # Validate given_sex input
  if given_sex not in ["male", "female"]:
      raise ValueError("given_sex must be either 'male' or 'female'")
      
  # Determine the field range based on sex
  if given_sex == "male":
    sex_range = range(3, 26)
  else:  # given_sex is "female"
    sex_range = range(27, 50)

  # Create field list for the API request
  field_list = ['NAME'] + [f'B01001_{str(i).zfill(3)}E' for i in sex_range]
  
  # Fetch data from the API
  ca_pop = c.acs5.state_zipcode(
    fields = tuple(field_list), 
    state_fips = '06',  
    zcta = '*',
    year = given_year
    )

  ca_pop = pd.DataFrame(ca_pop)
  ca_pop = ca_pop.rename(columns={"zip code tabulation area": "ZIP_CODE"})
  ca_pop['ZIP_CODE'] = ca_pop['ZIP_CODE'].astype(int)

  # Define age group column names
  age_groups = {
      "Under 5 years": 1,
      "5-14 years": (2, 3),
      "15-24 years": (4, 8),
      "25-34 years": (9, 10),
      "35-44 years": (11, 12),
      "45-54 years": (13, 14),
      "55-64 years": (15, 17),
      "65-74 years": (18, 20),
      "75-84 years": (21, 22),
      "85 years and over": 23
  }

  # Aggregate age groups
  for group, cols in age_groups.items():
      if isinstance(cols, tuple):
          ca_pop[group] = ca_pop.iloc[:, cols[0]:cols[1] + 1].sum(axis=1)
      else:
          ca_pop[group] = ca_pop.iloc[:, cols]

  # Select relevant columns
  ca_pop = ca_pop[["ZIP_CODE"] + list(age_groups.keys())]
  ca_pop = ca_pop.astype(int)

  # Melt the DataFrame
  melted_pop = ca_pop.melt(
      id_vars=["ZIP_CODE"],
      var_name="Strata_Name",
      value_name="Population"
  ).reset_index(drop=True)

  # Order the categories
  order = list(age_groups.keys())
  melted_pop["Strata_Name"] = pd.Categorical(melted_pop["Strata_Name"], categories=order, ordered=True)
  melted_pop = melted_pop.sort_values(by=["ZIP_CODE", "Strata_Name"]).reset_index(drop=True)

  # Add the year column
  melted_pop['Year'] = given_year

  return melted_pop

def calculate_population_by_age(given_year):
  male_pop = calculate_population_by_age_per_sex('male', given_year)
  female_pop = calculate_population_by_age_per_sex('female', given_year)

  # Merge the two DataFrames on 'ZIP_CODE' and sum the populations
  total_pop = pd.merge(male_pop, female_pop, on=['ZIP_CODE', 'Strata_Name'], suffixes=('_male', '_female'))
  total_pop['Population'] = total_pop['Population_male'] + total_pop['Population_female']
  total_pop = total_pop[['ZIP_CODE', 'Strata_Name', 'Population']]
  
  # Add the year column
  total_pop['Year'] = given_year

  return total_pop

years = range(2011, 2023)

def calculate_population(given_year_range):
  # Collect all DataFrames in a list  
  population_list = [calculate_population_by_age(year) for year in given_year_range]
  
  # Concatenate all DataFrames at once
  population = pd.concat(population_list, ignore_index=True)
  
  # Remove zip codes with 0 population in all strata
  population = (population
                .groupby(['Year', 'ZIP_CODE'])
                .filter(lambda x: (x['Population'] != 0).any())
  )

  # keeps only zipcodes in all the years 
  zip_counts = population['ZIP_CODE'].value_counts() / 10
  num_years = len(given_year_range)
  valid_zips = zip_counts[zip_counts == num_years].index
  population = population[population['ZIP_CODE'].isin(valid_zips)]

  # i originally stopped here but the count data said there were deaths while acs said the population is 0

  return population

population_df = calculate_population(years)

```

# Getting 2000 US Standard Population
```{python}

# Path to the file
file_path = 'stdpop.18ages.txt'

# Initialize lists to store the parsed data
age_groups = []
populations = []

# Read the file and process the lines
with open(file_path, 'r') as file:
    for line in file:
        if line.startswith('204'):
            age_group = int(line[3:6])  # Extract age group (001 to 018)
            population = int(line[6:])  # Extract population
            age_groups.append(age_group)
            populations.append(population)

# Create a DataFrame
std2000 = pd.DataFrame({
    'Strata_Name': age_groups,
    '2000 US Standard Population': populations
})

# Map age group numbers to their corresponding age ranges
age_group_map = {
    1: 'Under 5 years',
    2: '5-14 years', 3: '5-14 years', # 2 actually maps to 5-9 and 3 maps to 10-14 but this is done to aggregate later
    4: '15-24 years', 5: '15-24 years',
    6: '25-34 years', 7: '25-34 years',
    8: '35-44 years', 9: '35-44 years',
    10: '45-54 years', 11: '45-54 years',
    12: '55-64 years', 13: '55-64 years',
    14: '65-74 years', 15: '65-74 years',
    16: '75-84 years', 17: '75-84 years',
    18: '85 years and over'
}

std2000['Strata_Name'] = std2000['Strata_Name'].map(age_group_map)

# Aggregate the population by the new age groups
std2000 = std2000.groupby('Strata_Name', as_index=False, sort = False)['2000 US Standard Population'].sum()

# Display the DataFrame
display(std2000)

```

# Setting up mortality data for regressions
```{python}

# join population and mortality_age on zip code
test = pd.merge(mortality_age, population_df, on=["ZIP_CODE", "Year", "Strata_Name"], how="inner")

# handling cases where the population is less than the count
test.loc[test['Count'] > test['Population'], 'Population'] = test['Count'] * 2

# adding standard population
test = pd.merge(test, std2000, on='Strata_Name', how='left')

# 2301 values are the ones imputed with 5 (assuming imputation of 5)
# 7 values have mortality count of 11
# 3 values of mortality count of 12,13,15
# 2 values of mortality count of 14
# 1 value of mortality count of 22
# so we're setting the values where population is less than count to the population (0)

test['Mortality_Rate'] = test['Count'] / test['Population']
test['Expected Count'] = test['2000 US Standard Population'] * test['Mortality_Rate']

total_test = (
    test.groupby(['Year', 'ZIP_CODE'])
    .agg({
      'Count': 'sum', 
      'Population': 'sum', 
      '2000 US Standard Population': 'sum', 
      'Expected Count': 'sum'
      })
    .reset_index()
)

total_test['Mortality_Rate'] = total_test['Count'] / total_test['Population']
total_test['Adjusted_Mortality_Rate'] = total_test['Expected Count'] / total_test['2000 US Standard Population']

# var of expected count is greater than mean of expected count but other way around when looking at adjusted mortality rate


```

```{python}

def get_analyte_concentration(water, given_analyte):
  analyte_water = (water
                  .loc[water['Analyte Name'] == given_analyte]
                  .groupby(['Sample Year', 'Water System Number'])['Result']
                  .median()
                  .reset_index()
                  .rename(columns={"Water System Number": "WATER_SYST", "Sample Year": "Year"})
                  .sort_values(by=['WATER_SYST', 'Year'], ascending=[True, True])
                  .reset_index(drop=True)
  )
  return analyte_water

def cleaning_data(mortality_data, given_analyte):
  analyte_water = get_analyte_concentration(water, given_analyte)

  complete = (pd
              .merge(mortality_data, water_zip, on="ZIP_CODE", how="inner")
              .merge(analyte_water, how='inner', on=['WATER_SYST', 'Year'])
              .drop(columns=['WATER_SYST'])
              .assign(intercept=1)
  )

  return complete

```

# IGNORE BELOW THIS LINE AND CONTINUE FROM THE QMD FILE

# Regressions
```{python}

# join mortality_total and water_zip on zip code
# this creates a dataframe where zip_codes with multiple water systems are repeated for each water system and their respective (same) count

def get_analyte_concentration(water, given_analyte):
  analyte_water = (water
                  .loc[water['Analyte Name'] == given_analyte]
                  .groupby(['Sample Year', 'Water System Number'])['Result']
                  .median()
                  .reset_index()
                  .rename(columns={"Water System Number": "WATER_SYST", "Sample Year": "Year"})
                  .sort_values(by=['WATER_SYST', 'Year'], ascending=[True, True])
                  .reset_index(drop=True)
  )
  return analyte_water

def run_regression(regression, mortality_measurement, mortality_data, given_analyte, offset):
  analyte_water = get_analyte_concentration(water, given_analyte)

  complete = (pd
              .merge(mortality_data, water_zip, on="ZIP_CODE", how="inner")
              .merge(analyte_water, how='inner', on=['WATER_SYST', 'Year'])
              .drop(columns=['WATER_SYST'])
              .assign(intercept=1)
  )

  if regression == 'twfe':
    formula = f"{mortality_measurement} ~ intercept + Result + EntityEffects + TimeEffects"
    twfe_model = PanelOLS.from_formula(
      formula,
      data=complete.set_index(["ZIP_CODE", "Year"])
      ).fit(cov_type='clustered', cluster_entity=True)
    return twfe_model

  elif (regression == 'poisson') | (regression == 'nb2'):
    formula = f'{mortality_measurement} ~ intercept + Result + C(Year)'

    poisson_model = smf.glm(
      formula, 
      offset=np.log(complete[offset]),
      family=sm.families.Poisson(), 
      data=complete
    ).fit()

    if regression == 'poisson':
      return poisson_model

    elif regression == 'nb2':
      complete['LAMBDA'] = poisson_model.mu
      complete['AUX_OLS_DEP'] = complete.apply(lambda x: ((x['Count'] - x['LAMBDA'])**2 - x['LAMBDA']) / x['LAMBDA'], axis=1)      

      alpha_model = sm.OLS(complete['AUX_OLS_DEP'], complete['Result']).fit()

      if alpha_model.pvalues.iloc[0] < 0.01:
        alpha = alpha_model.params.iloc[0]
      else:
        alpha = 0 # poisson

      nb2_model = smf.glm(
        formula, 
        offset=np.log(complete[offset]),
        family=sm.families.NegativeBinomial(alpha=alpha), 
        data=complete
      ).fit()
      return nb2_model

  else:
    raise ValueError("Invalid regression type. Choose from 'twfe', 'poisson', or 'nb2'.")

run_regression('twfe', 'Count', total_test, 'LEAD', 'Population')

get_analyte_concentration(water, 'LEAD')


```

```{python}

analytes = water['Analyte Name'].unique()[0:5]
offset = 'Population'
mortality_data = total_test.copy()

def collect_regression_results(regression, mortality_measurement, mortality_data, analytes, offset):
  results = []
  
  for analyte in analytes:
    try:
      model = run_regression(regression, mortality_measurement, mortality_data, analyte, offset)
      coefficient = model.params['Result']
      p_value = model.pvalues['Result']
      conf_int = model.conf_int().loc['Result']

      if regression == 'twfe':
        std_error = model.std_errors['Result']
        t_stat = model.tstats['Result']
      else:
        std_error = model.bse['Result']
        t_stat = model.tvalues['Result']
      
      results.append({
        'Analyte': analyte,
        'Coefficient': coefficient,
        'Standard Error': std_error,
        't-statistic': t_stat,
        'p-value': p_value,
        'CI Lower Bound': conf_int.iloc[0],
        'CI Upper Bound': conf_int.iloc[1]
      })
    except Exception as e:
      # Handle any exceptions, such as missing data or convergence issues
      print(f"An error occurred for analyte {analyte}: {e}")
      continue
  
  return pd.DataFrame(results)

analytes = water['Analyte Name'].unique()

twfe_results = collect_regression_results('twfe', 'Adjusted_Mortality_Rate', mortality_data, analytes, offset)
# poisson_results = collect_regression_results('poisson', 'Count', mortality_data, analytes, offset)
# nb2_results = collect_regression_results('nb2', 'Count', mortality_data, analytes, offset)


water.loc[water['Units of Measure'] == 'PH', 'Reporting Level'].unique()
```

```{python}
poisson_results = collect_regression_results('poisson', 'Count', mortality_data, analytes, offset)
```

```{python}
nb2_results = collect_regression_results('nb2', 'Count', mortality_data, analytes, offset)
```


```{python}

# what to do about PH level of 0 since the reporting level is also 0 
# when looking at outliers, the value we set using reporting level is the max a lot of the time

top_analytes = water['Analyte Name'].unique()[:5]

for analyte in top_analytes:
  analyte_data = water[water['Analyte Name'] == analyte]
  top_rows = analyte_data.nlargest(5, 'Result')
  print(f"Top 5 rows for {analyte}:")
  display(top_rows)


```

# zip code problem
```{python}
# Compare the unique zip codes in mortality_total and zip_geo
mortality_zip_codes = mortality_total["ZIP_CODE"].unique()
zip_geo_codes = zip_geo["ZIP_CODE"].unique()

# Find the zip codes that are in mortality_total but not in zip_geo
missing_zip_codes = set(mortality_zip_codes) - set(zip_geo_codes)

# Find the zip codes that are in zip_geo but not in mortality_total
extra_zip_codes = set(zip_geo_codes) - set(mortality_zip_codes)

# Print the missing and extra zip codes
print("Missing zip codes:", missing_zip_codes)
print("Extra zip codes:", extra_zip_codes)
print(len(missing_zip_codes))
print(len(extra_zip_codes))

```

